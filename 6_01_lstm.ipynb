{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "import re\n",
    "#https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html\n",
    "#부분 참고\n",
    "\n",
    "class textDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,seq_length=20,filename = \"./data/aesop/data.txt\"):\n",
    "        self.filename= filename\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indices = [self.word_to_index[w] for w in self.words]\n",
    "        print(self.words_indices[:30])\n",
    "        self.len_indices =len(self.index_to_word)\n",
    "        self.len_text = len(self.words_indices)\n",
    "\n",
    "    def load_words(self):\n",
    "        with open(self.filename, encoding='utf-8-sig') as f:\n",
    "            text = f.read()\n",
    "        #removing text before and after the main stories\n",
    "        start = text.find(\"THE FOX AND THE GRAPES\\n\\n\\n\")\n",
    "        end = text.find(\"ILLUSTRATIONS\\n\\n\\n[\")\n",
    "        text = text[start:end]\n",
    "\n",
    "        start_story = '| ' * self.seq_length\n",
    "        text = start_story + text\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n\\n\\n\\n\\n', start_story)\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub('  +', '. ', text).strip()\n",
    "        text = text.replace('..', '.')\n",
    "\n",
    "        text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text)\n",
    "        text = re.sub('\\s{2,}', ' ', text)\n",
    "        ## 맨 앞에 ' '가 있음\n",
    "        return text[1:].split(' ')\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_text - self.seq_length\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        #one-hot\n",
    "        y = self.words_indices[index+self.seq_length]\n",
    "        one_hot_label = torch.tensor(np.eye(self.len_indices)[y])\n",
    "        return (\n",
    "            torch.tensor(self.words_indices[index:index+self.seq_length]),\n",
    "            one_hot_label,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import RMSprop,Adam\n",
    "\n",
    "class textGenLSTM(nn.Module):\n",
    "    def __init__(self,seq_length=20,total_words=4169):\n",
    "        super().__init__()\n",
    "        \n",
    "        # n_units = 256\n",
    "        # embedding_size = 100\n",
    "        self.seq_length =20\n",
    "        self.n_units = 256\n",
    "        self.num_layers = 1 #적층 레이어를 위해\n",
    "        embedding_size = 256\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=total_words, embedding_dim= embedding_size\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.n_units,\n",
    "            hidden_size=self.n_units,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,            \n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_units,total_words),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(),lr=0.001)\n",
    "\n",
    "    def forward(self,x,prev_state): \n",
    "        #[32, 20]\n",
    "        emb = self.embedding(x) \n",
    "        #[32, 20, 256]\n",
    "        # h0 [1, 32, 256], c0[1, 32, 256]\n",
    "        output,state = self.lstm(emb,prev_state)\n",
    "        # [32, 20, 256], ([1, 32, 256],[1, 32, 256])\n",
    "        logits = self.fc(output[:,-1,:]) \n",
    "        #[32, 4169]\n",
    "        return logits,state\n",
    "    \n",
    "    def init_state(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.n_units),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.n_units))\n",
    "    \n",
    "    def train(self,dataloader):\n",
    "        \n",
    "        for iter, (x, y) in enumerate(dataloader):\n",
    "            self.batch_size = x.shape[0]\n",
    "            state_h, state_c = self.init_state()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            y_pred, (state_h, state_c) = self.forward(x, (state_h, state_c))\n",
    "            loss = self.loss_fn(y_pred, y)\n",
    "\n",
    "            state_h = state_h.detach() \n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if iter%100==0:\n",
    "                print(iter, f\"loss {loss.item():.4f}\")\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 55, 3, 2, 939, 4, 5, 381, 55, 93]\n"
     ]
    }
   ],
   "source": [
    "seq_length=20\n",
    "my_dataset = textDataset(seq_length=seq_length)\n",
    "total_indexing_num = len(my_dataset.word_to_index)\n",
    "model = textGenLSTM(seq_length=20,total_words=total_indexing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8.3354\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n",
      "x.shape torch.Size([32, 20])\n",
      "emb.shape torch.Size([32, 20, 256])\n",
      "prev_state.shape torch.Size([1, 32, 256])\n",
      "output.shape torch.Size([32, 20, 256])\n",
      "state.shape torch.Size([1, 32, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(my_dataset, batch_size\u001b[39m=\u001b[39mbatch_size,shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 14\u001b[0m     loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(dataloader)\n\u001b[0;32m     16\u001b[0m     \u001b[39mprint\u001b[39m({ \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch, \u001b[39m'\u001b[39m\u001b[39miter\u001b[39m\u001b[39m'\u001b[39m: \u001b[39miter\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: loss\u001b[39m.\u001b[39mitem() })\n",
      "Cell \u001b[1;32mIn[14], line 71\u001b[0m, in \u001b[0;36mtextGenLSTM.train\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     68\u001b[0m state_h \u001b[39m=\u001b[39m state_h\u001b[39m.\u001b[39mdetach() \n\u001b[0;32m     69\u001b[0m state_c \u001b[39m=\u001b[39m state_c\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m---> 71\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39miter\u001b[39m\u001b[39m%\u001b[39m\u001b[39m100\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "num_epochs=10\n",
    "\n",
    "dataloader = DataLoader(my_dataset, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    \n",
    "    loss = model.train(dataloader)\n",
    "\n",
    "    print({ 'epoch': epoch, 'iter': iter, 'loss': loss.item() })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_words(input_text,dataset):\n",
    "    text = input_text\n",
    "\n",
    "    start_story = '| ' * seq_length\n",
    "    text = start_story + text\n",
    "    text = text.lower()\n",
    "    words =  text[1:].split(' ')\n",
    "    words_indices = [dataset.word_to_index[w] for w in words]\n",
    "    return words_indices\n",
    "\n",
    "def sample_with_temp(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len=20, temp=0.2,dataset=my_dataset):\n",
    "    output_text = seed_text\n",
    "    \n",
    "    seed_text = '| ' * seq_length + seed_text\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        token_list = load_words(seed_text,dataset)\n",
    "        token_list = token_list[-max_sequence_len:]\n",
    "        token_list = np.reshape(token_list, (1, max_sequence_len))\n",
    "        token_list = torch.tensor(token_list)\n",
    "\n",
    "        state_h, state_c = model.init_state()\n",
    "        probs = model.forward(token_list,(state_h, state_c))[0].detach().numpy()\n",
    "        y_class = sample_with_temp(probs[0], temperature = temp)\n",
    "        \n",
    "        if y_class == 0:\n",
    "            output_word = ''\n",
    "        else:\n",
    "            output_word = dataset.index_to_word[y_class]\n",
    "            \n",
    "        if output_word == \"|\":\n",
    "            break\n",
    "            \n",
    "\n",
    "        output_text += output_word + ' '\n",
    "        seed_text += output_word + ' '\n",
    "\n",
    "            \n",
    "            \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the frog and the snake . earlier strong gnat gained antics however harm astonishment revives loaded blow few judgment feast tan despair heaven driving profession watched precautions idea a crops sky stall plunged somehow laughter served country figure repaired sharing share figure bowl felt baring appoint shed discovery immense younger pursuer wag shot angrily work grotesque clear leave lasting overflowing strutted unless dug assembly neighbouring wasn't harvest herdsmen avenge summer's bridle right doomed eh pay alighted then quite running blocked steps manner brayed still gaily berries dug edge sharp profit sweet tunes trained several upper visits kindness flocks passage time possibly gained perished wings ahead fierce \n"
     ]
    }
   ],
   "source": [
    "seed_text = \"the frog and the snake . \"\n",
    "gen_words = 100\n",
    "print (generate_text(seed_text, gen_words, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "747fe91b7bc9ec9d8624ac8c139c41948fb906c2c40d5ffbc4d71da454373257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
