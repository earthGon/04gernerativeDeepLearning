{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "from os import walk\n",
    "\n",
    "def load_safari(folder):\n",
    "\n",
    "    mypath = os.path.join(\"./data\", folder)\n",
    "    txt_name_list = []\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        for f in filenames:\n",
    "            if f != '.DS_Store':\n",
    "                txt_name_list.append(f)\n",
    "                break\n",
    "\n",
    "    slice_train = int(80000/len(txt_name_list))  ###Setting value to be 80000 for the final dataset\n",
    "    i = 0\n",
    "    seed = np.random.randint(1, 10e6)\n",
    "\n",
    "    for txt_name in txt_name_list:\n",
    "        txt_path = os.path.join(mypath,txt_name)\n",
    "        x = np.load(txt_path)\n",
    "        x = (x.astype('float32') - 127.5) / 127.5\n",
    "        # x = x.astype('float32') / 255.0\n",
    "        \n",
    "        x = x.reshape(x.shape[0], 28, 28, 1)\n",
    "        \n",
    "        y = [i] * len(x)  \n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(x)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(y)\n",
    "        x = x[:slice_train]\n",
    "        y = y[:slice_train]\n",
    "        if i != 0: \n",
    "            xtotal = np.concatenate((x,xtotal), axis=0)\n",
    "            ytotal = np.concatenate((y,ytotal), axis=0)\n",
    "        else:\n",
    "            xtotal = x\n",
    "            ytotal = y\n",
    "        i += 1\n",
    "        \n",
    "    return xtotal, ytotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self,shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape,\n",
    "    def forward(self,x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad,Variable\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self,dim_latent=200):\n",
    "        # 파이토치 부모 클래스 초기화\n",
    "        super().__init__()\n",
    "        self.dim_latent = dim_latent\n",
    "       # 신경망 레이어 정의\n",
    "        self.model = nn.Sequential(\n",
    "            #(-1,1,28,28)\n",
    "            nn.Conv2d(1, 64, kernel_size=7,stride=2,padding=3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,64,14,14)\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=5, stride=2,padding=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,64,7,7)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,128,4,4)\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,128,4,4)\n",
    "\n",
    "            View((-1,128*4*4)),\n",
    "            nn.Linear(128*4*4, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "        # 손실 함수 생성\n",
    "        \n",
    "        # 옵티마이저 생성\n",
    "        self.optimiser = torch.optim.Adam(self.parameters(), lr=0.0005)\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def wasserstein_loss(self,y_pred,y_true):\n",
    "        return torch.mean(y_true*y_pred)\n",
    "\n",
    "    def gradient_penalty_loss(self,x_batch,fake_img):\n",
    "        batch_size = x_batch.shape[0]\n",
    "\n",
    "        # Calculate interpolation\n",
    "        alpha = torch.rand(batch_size, 1, 1, 1)\n",
    "        alpha = alpha.expand_as(x_batch)\n",
    "        \n",
    "        #alpha = alpha.cuda()\n",
    "        interpolated = alpha * x_batch.data + (1 - alpha) * fake_img.data\n",
    "        interpolated = Variable(interpolated, requires_grad=True)        \n",
    "        #interpolated=interpolated.cuda()\n",
    "        y_pred = self.forward(interpolated)\n",
    "\n",
    "        gradients = grad(outputs=y_pred, inputs=interpolated,\n",
    "                               grad_outputs=torch.ones(y_pred.size()).cuda() if torch.cuda.is_available() else torch.ones(\n",
    "                               y_pred.size()),\n",
    "                               create_graph=True, retain_graph=True)[0] #[64,1,28,28]\n",
    "\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "        gp_loss = ((gradients_norm - 1) ** 2).mean()\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "        gp_loss.backward()\n",
    "        self.optimiser.step()\n",
    "        return gp_loss\n",
    "\n",
    "    def train(self,x_batch,label):\n",
    "        self.optimiser.zero_grad()\n",
    "        y_pred = self.forward(x_batch)\n",
    "\n",
    "        \n",
    "\n",
    "        loss = self.wasserstein_loss(y_pred,label)\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        return loss\n",
    "        ## weight clipping하면 clipped grad 어떻게 계산되지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self,dim_latent=100):\n",
    "        # 파이토치 부모 클래스 초기화\n",
    "        super().__init__()\n",
    "       # 신경망 레이어 정의\n",
    "        self.dim_latent = dim_latent\n",
    "        self.model = nn.Sequential(\n",
    "            #(-1,100)\n",
    "            nn.Linear(self.dim_latent,64*7*7),\n",
    "            View((-1,64,7,7)),\n",
    "            #(-1,3136)\n",
    "            #(-1,64,7,7)\n",
    "            \n",
    "            nn.ConvTranspose2d(64,64,4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),            \n",
    "            #(-1,64,14,14)\n",
    "\n",
    "            nn.ConvTranspose2d(64,128,5,padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,128,14,14)\n",
    "\n",
    "            nn.ConvTranspose2d(128,64,6,stride=2,padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,64,28,28)\n",
    "\n",
    "            nn.ConvTranspose2d(64,1,5,padding=2),\n",
    "            nn.Tanh(),\n",
    "            #(-1,1,28,28)\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "        # 옵티마이저 생성\n",
    "        self.optimiser = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train(self,D,latent_batch,label):\n",
    "    \n",
    "        self.optimiser.zero_grad()\n",
    "        ## D.optimiser.zero_grad()  필요없음\n",
    "        g_output = self.forward(latent_batch)\n",
    "        d_output = D(g_output)\n",
    "\n",
    "        loss = D.wasserstein_loss(d_output,label)\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        # D(x_input,1) ->D학습\n",
    "        # D(G(latent), 0) ->D학습\n",
    "        # D(G(latent),1) ->G학습\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,foldername='camel'):\n",
    "        (self.x_train, self.y_train) =load_safari(foldername)\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x_train = self.x_train.transpose((0,3,1,2))\n",
    "        return torch.FloatTensor(x_train[idx])\n",
    "        #return torch.cuda.FloatTensor(self.x_train[idx])\n",
    "#data_loader = DataLoader(MyDataset(), batch_size=128, shuffle=True,generator=torch.Generator(device='cuda'))\n",
    "data_loader = DataLoader(MyDataset(), batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1174, -0.0538, -0.1155, -0.0917,  0.0879, -0.0789,  0.0230],\n",
      "         [ 0.0156, -0.0600, -0.0615,  0.0473, -0.0534,  0.0918, -0.0627],\n",
      "         [-0.0437,  0.0755, -0.0291, -0.1245,  0.1260, -0.1191,  0.0248],\n",
      "         [-0.0515,  0.0629,  0.1147, -0.0463,  0.0073, -0.0604, -0.0442],\n",
      "         [-0.0516,  0.1370,  0.0958,  0.0421,  0.0810, -0.1087, -0.0011],\n",
      "         [-0.0214, -0.0704,  0.1094, -0.1189, -0.1049, -0.0863,  0.0132],\n",
      "         [-0.0530, -0.1027,  0.0043,  0.0497, -0.0191, -0.1281, -0.0105]]],\n",
      "       requires_grad=True)\n",
      "<class 'tuple'> 1\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m fake_img_batch \u001b[39m=\u001b[39m g(torch\u001b[39m.\u001b[39mrandn(\u001b[39m64\u001b[39m,\u001b[39m100\u001b[39m))\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     16\u001b[0m d_fake_loss\u001b[39m=\u001b[39md\u001b[39m.\u001b[39mtrain(fake_img_batch,fake_label)\n\u001b[1;32m---> 17\u001b[0m gp_loss \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39;49mgradient_penalty_loss(x_input,fake_img_batch)\n\u001b[0;32m     18\u001b[0m \u001b[39m# gp_loss(fake_img_batch, x_input)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m## interpolated_img = interpolate_img(fake_img_batch,x_input)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m## d.optimizer.zero_grad()\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m## d.forward(interpolated_img)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m## gradient 다 모으고\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m## L2_norm ==1 되도록 loss\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 69\u001b[0m, in \u001b[0;36mD.gradient_penalty_loss\u001b[1;34m(self, x_batch, fake_img)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(gradients),\u001b[39mlen\u001b[39m(gradients))\n\u001b[0;32m     68\u001b[0m \u001b[39mprint\u001b[39m(gradients[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mprint\u001b[39m(gradients[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     70\u001b[0m \u001b[39mprint\u001b[39m(gradients[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     71\u001b[0m \u001b[39mprint\u001b[39m(gradients[\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "d = D()\n",
    "g= G()\n",
    "#d.to(device)\n",
    "#g.to(device)\n",
    "\n",
    "d_loss_hist = []\n",
    "g_loss_hist =[]\n",
    "for epoch in range(1, 101):\n",
    "    for i,x_input in enumerate(data_loader):\n",
    "\n",
    "        pass\n",
    "        label = torch.ones((x_input.shape[0],1))\n",
    "        fake_label = -torch.ones((x_input.shape[0],1))\n",
    "        d_loss=d.train(x_input,label)\n",
    "        fake_img_batch = g(torch.randn(64,100)).detach()\n",
    "        d_fake_loss=d.train(fake_img_batch,fake_label)\n",
    "        gp_loss = d.gradient_penalty_loss(x_input,fake_img_batch)\n",
    "        # gp_loss(fake_img_batch, x_input)\n",
    "        ## interpolated_img = interpolate_img(fake_img_batch,x_input)\n",
    "        ## d.optimizer.zero_grad()\n",
    "        ## d.forward(interpolated_img)\n",
    "        ## gradient 다 모으고\n",
    "        ## L2_norm ==1 되도록 loss\n",
    "        break\n",
    "    break\n",
    "    #     total_d_loss =(d_loss.item()+d_fake_loss.item()+gp_loss.item())/2\n",
    "    #     d_loss_hist.append(total_d_loss)\n",
    "    #     if (i%5==4):\n",
    "    #         g_loss=g.train(d,torch.randn(64,100),label)\n",
    "    #         g_loss_hist.append(g_loss.item())\n",
    "    #         print(f\"Epoch {epoch}: d_loss = {total_d_loss:.9f}  g loss={g_loss.item():.9f} gp loss={gp_loss.item():.9f}\")\n",
    "    # if epoch % 1 == 0:\n",
    "    #     print(f\"Epoch {epoch}: d_loss = {total_d_loss:.9f}  g loss={g_loss.item():.9f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fake_pred = g.forward(torch.randn(8,100)).detach().cpu().numpy() #(8,1,28,28)\n",
    "fake_pred = fake_pred.transpose(0,2,3,1) #(8,28,28,1)\n",
    "fake_8img = fake_pred.transpose(1,0,2,3).reshape(28,-1,1)\n",
    "plt.figure(figsize=(16,2))\n",
    "plt.imshow(fake_8img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(loss_hist):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.scatter(np.arange(1,len(loss_hist)+1),loss_hist,s=0.5)\n",
    "    plt.title('generator loss')\n",
    "    plt.show()\n",
    "visualize(g_loss_hist)\n",
    "visualize(d_loss_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "747fe91b7bc9ec9d8624ac8c139c41948fb906c2c40d5ffbc4d71da454373257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
