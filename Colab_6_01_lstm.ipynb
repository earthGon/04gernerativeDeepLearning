{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 데이터 전처리 부분은 책 코드 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpmNuQ9VKlHj"},"outputs":[],"source":["import numpy as np\n","import torch\n","from collections import Counter\n","import re\n","#https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html\n","#부분 참고\n","\n","class textDataset(torch.utils.data.Dataset):\n","    def __init__(self,seq_length=20,filename = \"./data/aesop/data.txt\"):\n","        self.filename= filename\n","\n","        self.seq_length = seq_length\n","        self.words = self.load_words()\n","        self.uniq_words = self.get_uniq_words()\n","\n","        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n","        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n","\n","        self.words_indices = [self.word_to_index[w] for w in self.words]\n","        print(self.words_indices[:30])\n","        self.len_indices =len(self.index_to_word)\n","        self.len_text = len(self.words_indices)\n","\n","    def load_words(self):\n","        with open(self.filename, encoding='utf-8-sig') as f:\n","            text = f.read()\n","        #removing text before and after the main stories\n","        start = text.find(\"THE FOX AND THE GRAPES\\n\\n\\n\")\n","        end = text.find(\"ILLUSTRATIONS\\n\\n\\n[\")\n","        text = text[start:end]\n","\n","        start_story = '| ' * self.seq_length\n","        text = start_story + text\n","        text = text.lower()\n","        text = text.replace('\\n\\n\\n\\n\\n', start_story)\n","        text = text.replace('\\n', ' ')\n","        text = re.sub('  +', '. ', text).strip()\n","        text = text.replace('..', '.')\n","\n","        text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text)\n","        text = re.sub('\\s{2,}', ' ', text)\n","        ## 맨 앞에 ' '가 있음\n","        return text[1:].split(' ')\n","\n","    def get_uniq_words(self):\n","        word_counts = Counter(self.words)\n","        return sorted(word_counts, key=word_counts.get, reverse=True)\n","\n","    def __len__(self):\n","        return self.len_text - self.seq_length\n","\n","    \n","    def __getitem__(self, index):\n","        \n","        #one-hot\n","        y = self.words_indices[index+self.seq_length]\n","        one_hot_label = torch.tensor(np.eye(self.len_indices)[y])\n","        return (\n","            torch.tensor(self.words_indices[index:index+self.seq_length]),\n","            one_hot_label,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKDe58zzKlEM"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import RMSprop,Adam\n","\n","class textGenLSTM(nn.Module):\n","    def __init__(self,seq_length=20,total_words=4169):\n","        super().__init__()\n","        \n","        # n_units = 256\n","        # embedding_size = 100\n","        self.seq_length =20\n","        self.n_units = 256\n","        self.num_layers = 1 #적층 레이어를 위해\n","        embedding_size = 256\n","\n","        self.embedding = nn.Embedding(\n","            num_embeddings=total_words, embedding_dim= embedding_size\n","        )\n","\n","        self.lstm = nn.LSTM(\n","            input_size=self.n_units,\n","            hidden_size=self.n_units,\n","            num_layers=self.num_layers,\n","            batch_first=True,\n","            dropout=0.2,            \n","        )\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(self.n_units,total_words),\n","            nn.Softmax(),\n","        )\n","        \n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.optimizer = Adam(self.parameters(),lr=0.001)\n","\n","    def forward(self,x,prev_state): \n","        #[32, 20]\n","        emb = self.embedding(x) \n","        #[32, 20, 256]\n","        # h0 [1, 32, 256], c0[1, 32, 256]\n","        output,state = self.lstm(emb,prev_state)\n","        # [32, 20, 256], ([1, 32, 256],[1, 32, 256])\n","        logits = self.fc(output[:,-1,:]) \n","        #[32, 4169]\n","        return logits,state\n","    \n","    def init_state(self):\n","        return (torch.zeros(self.num_layers, self.batch_size, self.n_units),\n","                torch.zeros(self.num_layers, self.batch_size, self.n_units))\n","    \n","    def train(self,dataloader):\n","        \n","        for iter, (x, y) in enumerate(dataloader):\n","            self.batch_size = x.shape[0]\n","            state_h, state_c = self.init_state()\n","\n","            self.optimizer.zero_grad()\n","            \n","            y_pred, (state_h, state_c) = self.forward(x, (state_h, state_c))\n","            loss = self.loss_fn(y_pred, y)\n","\n","            state_h = state_h.detach() \n","            state_c = state_c.detach()\n","\n","            loss.backward()\n","            self.optimizer.step()\n","            if iter%100==0:\n","                print(iter, f\"loss {loss.item():.4f}\")\n","        return loss\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8CVLwNgKnS_"},"outputs":[],"source":["seq_length=20\n","my_dataset = textDataset(seq_length=seq_length)\n","total_indexing_num = len(my_dataset.word_to_index)\n","model = textGenLSTM(seq_length=20,total_words=total_indexing_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXQ4fw-9KnPg"},"outputs":[],"source":["import numpy as np\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","\n","\n","batch_size=32\n","num_epochs=10\n","\n","dataloader = DataLoader(my_dataset, batch_size=batch_size,shuffle=True)\n","\n","\n","for epoch in range(1,num_epochs+1):\n","    \n","    loss = model.train(dataloader)\n","\n","    print({ 'epoch': epoch, 'iter': iter, 'loss': loss.item() })\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1RuTLQ2KlA6"},"outputs":[],"source":["\n","def load_words(input_text,dataset):\n","    text = input_text\n","\n","    start_story = '| ' * seq_length\n","    text = start_story + text\n","    text = text.lower()\n","    words =  text[1:].split(' ')\n","    words_indices = [dataset.word_to_index[w] for w in words]\n","    return words_indices\n","\n","def sample_with_temp(preds, temperature=1.0):\n","    # helper function to sample an index from a probability array\n","    preds = np.asarray(preds).astype('float64')\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)\n","\n","\n","\n","def generate_text(seed_text, next_words, model, max_sequence_len=20, temp=0.2,dataset=my_dataset):\n","    output_text = seed_text\n","    \n","    seed_text = '| ' * seq_length + seed_text\n","    \n","    for _ in range(next_words):\n","        token_list = load_words(seed_text,dataset)\n","        token_list = token_list[-max_sequence_len:]\n","        token_list = np.reshape(token_list, (1, max_sequence_len))\n","        token_list = torch.tensor(token_list)\n","\n","        state_h, state_c = model.init_state()\n","        probs = model.forward(token_list,(state_h, state_c))[0].detach().numpy()\n","        y_class = sample_with_temp(probs[0], temperature = temp)\n","        \n","        if y_class == 0:\n","            output_word = ''\n","        else:\n","            output_word = dataset.index_to_word[y_class]\n","            \n","        if output_word == \"|\":\n","            break\n","            \n","\n","        output_text += output_word + ' '\n","        seed_text += output_word + ' '\n","\n","            \n","            \n","    return output_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"522rUbd1KsTR"},"outputs":[],"source":["seed_text = \"the frog and the snake . \"\n","gen_words = 100\n","print (generate_text(seed_text, gen_words, model))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vN8AmIDdKsRL"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPYo9KYWX1olulx+kypQZJR","provenance":[]},"kernelspec":{"display_name":"torch","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.16"},"vscode":{"interpreter":{"hash":"747fe91b7bc9ec9d8624ac8c139c41948fb906c2c40d5ffbc4d71da454373257"}}},"nbformat":4,"nbformat_minor":0}
