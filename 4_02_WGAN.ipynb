{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "from os import walk\n",
    "\n",
    "def load_safari(folder):\n",
    "\n",
    "    mypath = os.path.join(\"./data\", folder)\n",
    "    txt_name_list = []\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        for f in filenames:\n",
    "            if f != '.DS_Store':\n",
    "                txt_name_list.append(f)\n",
    "                break\n",
    "\n",
    "    slice_train = int(80000/len(txt_name_list))  ###Setting value to be 80000 for the final dataset\n",
    "    i = 0\n",
    "    seed = np.random.randint(1, 10e6)\n",
    "\n",
    "    for txt_name in txt_name_list:\n",
    "        txt_path = os.path.join(mypath,txt_name)\n",
    "        x = np.load(txt_path)\n",
    "        x = (x.astype('float32') - 127.5) / 127.5\n",
    "        # x = x.astype('float32') / 255.0\n",
    "        \n",
    "        x = x.reshape(x.shape[0], 28, 28, 1)\n",
    "        \n",
    "        y = [i] * len(x)  \n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(x)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(y)\n",
    "        x = x[:slice_train]\n",
    "        y = y[:slice_train]\n",
    "        if i != 0: \n",
    "            xtotal = np.concatenate((x,xtotal), axis=0)\n",
    "            ytotal = np.concatenate((y,ytotal), axis=0)\n",
    "        else:\n",
    "            xtotal = x\n",
    "            ytotal = y\n",
    "        i += 1\n",
    "        \n",
    "    return xtotal, ytotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self,shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape,\n",
    "    def forward(self,x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(nn.Module):\n",
    "    def __init__(self,dim_latent=200):\n",
    "        # 파이토치 부모 클래스 초기화\n",
    "        super().__init__()\n",
    "        self.dim_latent = dim_latent\n",
    "       # 신경망 레이어 정의\n",
    "        self.model = nn.Sequential(\n",
    "            #(-1,1,28,28)\n",
    "            nn.Conv2d(1, 64, kernel_size=7,stride=2,padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,64,14,14)\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=5, stride=2,padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,64,7,7)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,128,4,4)\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,128,4,4)\n",
    "\n",
    "            View((-1,128*4*4)),\n",
    "            nn.Linear(128*4*4, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "        # 손실 함수 생성\n",
    "        self.loss_function = self.wasserstein\n",
    "        # 옵티마이저 생성\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0005)\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def wasserstein(self,y_pred,y_true):\n",
    "        return torch.mean(y_true*y_pred)\n",
    "\n",
    "    def train(self,x_batch,label):\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred = self.forward(x_batch)\n",
    "        \n",
    "        loss = self.loss_function(y_pred,label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "        ## weight clipping하면 clipped grad 어떻게 계산되지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self,dim_latent=100):\n",
    "        # 파이토치 부모 클래스 초기화\n",
    "        super().__init__()\n",
    "       # 신경망 레이어 정의\n",
    "        self.dim_latent = dim_latent\n",
    "        self.model = nn.Sequential(\n",
    "            #(-1,100)\n",
    "            nn.Linear(self.dim_latent,64*7*7),\n",
    "            View((-1,64,7,7)),\n",
    "            #(-1,3136)\n",
    "            #(-1,64,7,7)\n",
    "            \n",
    "            nn.ConvTranspose2d(64,64,4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),            \n",
    "            #(-1,64,14,14)\n",
    "\n",
    "            nn.ConvTranspose2d(64,128,5,padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,128,14,14)\n",
    "\n",
    "            nn.ConvTranspose2d(128,64,6,stride=2,padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            #(-1,64,28,28)\n",
    "\n",
    "            nn.ConvTranspose2d(64,1,5,padding=2),\n",
    "            nn.Tanh(),\n",
    "            #(-1,1,28,28)\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "        # 옵티마이저 생성\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train(self,D,latent_batch,label):\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        ## D.optimiser.zero_grad()  필요없음\n",
    "        g_output = self.forward(latent_batch)\n",
    "        d_output = D(g_output)\n",
    "\n",
    "        loss = D.loss_function(d_output,label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # D(x_input,1) ->D학습\n",
    "        # D(G(latent), 0) ->D학습\n",
    "        # D(G(latent),1) ->G학습\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,foldername='camel'):\n",
    "        (self.x_train, self.y_train) =load_safari(foldername)\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x_train = self.x_train.transpose((0,3,1,2))\n",
    "        return torch.FloatTensor(x_train[idx])\n",
    "        #return torch.cuda.FloatTensor(self.x_train[idx])\n",
    "#data_loader = DataLoader(MyDataset(), batch_size=128, shuffle=True,generator=torch.Generator(device='cuda'))\n",
    "data_loader = DataLoader(MyDataset(), batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = D()\n",
    "g= G()\n",
    "#d.to(device)\n",
    "#g.to(device)\n",
    "\n",
    "d_loss_hist = []\n",
    "g_loss_hist =[]\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    for i,x_input in enumerate(data_loader):\n",
    "\n",
    "        pass\n",
    "        label = torch.ones((x_input.shape[0],1))\n",
    "        fake_label = -torch.ones((x_input.shape[0],1))\n",
    "        d_loss=d.train(x_input,label)\n",
    "        d_fake_loss=d.train(g(torch.randn(64,100)).detach(),fake_label)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in d.parameters():\n",
    "                param.clamp_(-0.01, 0.01)\n",
    "        \n",
    "\n",
    "        \n",
    "        total_d_loss =(d_loss.item()+d_fake_loss.item())/2\n",
    "        d_loss_hist.append(total_d_loss)\n",
    "        if (i%5==4):\n",
    "            g_loss=g.train(d,torch.randn(64,100),label)\n",
    "            g_loss_hist.append(g_loss.item())\n",
    "            print(f\"Epoch {epoch}: d_loss = {total_d_loss:.9f}  g loss={g_loss.item():.9f}\")\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}: d_loss = {total_d_loss:.9f}  g loss={g_loss.item():.9f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fake_pred = g.forward(torch.randn(8,100)).detach().cpu().numpy() #(8,1,28,28)\n",
    "fake_pred = fake_pred.transpose(0,2,3,1) #(8,28,28,1)\n",
    "fake_8img = fake_pred.transpose(1,0,2,3).reshape(28,-1,1)\n",
    "plt.figure(figsize=(16,2))\n",
    "plt.imshow(fake_8img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(loss_hist):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.scatter(np.arange(1,len(loss_hist)+1),loss_hist,s=0.5)\n",
    "    plt.title('generator loss')\n",
    "    plt.show()\n",
    "visualize(g_loss_hist)\n",
    "visualize(d_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39massert\u001b[39;00m x\u001b[39m==\u001b[39m\u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "747fe91b7bc9ec9d8624ac8c139c41948fb906c2c40d5ffbc4d71da454373257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
