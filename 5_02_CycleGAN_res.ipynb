{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatDownSampling(in_channels,out_channels,f_size=4):\n",
    "    ds = nn.Sequential(\n",
    "        nn.Conv2d(in_channels,out_channels,kernel_size=f_size,stride=2,padding=1),\n",
    "        nn.InstanceNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    return ds\n",
    "def residual(in_channels,f_size=3):\n",
    "    rs = nn.Sequential(\n",
    "        nn.Conv2d(in_channels,in_channels,kernel_size=f_size,stride=1,padding=1),\n",
    "        nn.InstanceNorm2d(in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels,in_channels,kernel_size=f_size,stride=1,padding=1),\n",
    "        nn.InstanceNorm2d(in_channels),\n",
    "    )\n",
    "    return rs\n",
    "\n",
    "def creatUpSampling(in_channels,out_channels,f_size=3,dropout_rate=0):\n",
    "    us = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2),\n",
    "        nn.Conv2d(in_channels,out_channels,kernel_size=f_size,stride=1,padding=1,),\n",
    "        nn.InstanceNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    if dropout_rate:\n",
    "        us = nn.Sequential(\n",
    "            us,\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    return us\n",
    "def conv7s1(in_channels,out_channels,f_size=7,final=True):\n",
    "    if final:\n",
    "        conv7s1_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=f_size,stride=1,padding=3),\n",
    "            nn.Tanh(),\n",
    "        )   \n",
    "    else:\n",
    "        conv7s1_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=f_size,stride=1,padding=3),\n",
    "            nn.InstanceNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "        )         \n",
    "    return conv7s1_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,filters=32):\n",
    "        super().__init__()\n",
    "        self.filters = filters\n",
    "\n",
    "        self.input = conv7s1(3,self.filters,final=False)        \n",
    "        self.ds1 = creatDownSampling(self.filters,2*self.filters,4)\n",
    "        self.ds2 = creatDownSampling(2*self.filters,4*self.filters,4)\n",
    "        self.rs1 = residual(4*self.filters)\n",
    "        self.rs2 = residual(4*self.filters)\n",
    "        self.rs3 = residual(4*self.filters)\n",
    "        self.rs4 = residual(4*self.filters)\n",
    "        self.rs5 = residual(4*self.filters)\n",
    "        self.rs6 = residual(4*self.filters)\n",
    "        self.rs7 = residual(4*self.filters)\n",
    "        self.rs8 = residual(4*self.filters)\n",
    "        self.rs9 = residual(4*self.filters)\n",
    "        self.us1 = creatUpSampling(4*self.filters,2*self.filters)\n",
    "        self.us2 = creatUpSampling(2*self.filters,self.filters)\n",
    "        self.output = conv7s1(self.filters,3,final=True)        \n",
    "        self.loss_function = nn.L1Loss()\n",
    "        self.optimizer = Adam(self.parameters(),lr=0.0005)\n",
    "    def forward(self,x): #[2,3,256,256]\n",
    "        x = self.input(x) #[2, 32, 256, 256]\n",
    "        d1 = self.ds1(x) #[2, 64, 128, 128]\n",
    "        d2 = self.ds2(d1)#[2, 128, 64, 64]\n",
    "        r1 = self.rs1(d2)#[2, 128, 64, 64]\n",
    "        r1 = torch.add(r1,d2)#[2, 128, 64, 64]\n",
    "        r2 = self.rs2(r1)\n",
    "        r2 = torch.add(r2,r1)\n",
    "        r3 = self.rs3(r2)\n",
    "        r3 = torch.add(r3,r2)\n",
    "        r4 = self.rs4(r3)\n",
    "        r4 = torch.add(r4,r3)\n",
    "        r5 = self.rs5(r4)\n",
    "        r5 =torch.add(r5,r4)\n",
    "        r6 = self.rs6(r5)\n",
    "        r6 =torch.add(r6,r5)\n",
    "        r7 = self.rs7(r6)\n",
    "        r7 =torch.add(r7,r6)\n",
    "        r8 = self.rs8(r7)\n",
    "        r8 = torch.add(r8,r7)\n",
    "        r9 = self.rs9(r8)\n",
    "        r9 = torch.add(r9,r8) #[2, 128, 64, 64]       \n",
    "        \n",
    "        u1 = self.us1(r8) #[2, 64, 128, 128]\n",
    "        u2 = self.us2(u1) #[2, 32, 256, 256]\n",
    "        output = self.output(u2) #[2, 3, 256, 256]\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def train(self,x_batch,label_imgs):\n",
    "        self.optimizer.zero_grad()\n",
    "        fake_imgs = self.forward(x_batch)\n",
    "\n",
    "        loss = self.loss_function(fake_imgs,label_imgs)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.ones([2,3,256,256])\n",
    "resnet = ResNet()\n",
    "resnet(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [-1,3,128,128] -> [-1,32, 64,64] -> [-1,64,32,32]->[-1,128,16,16]->[-1,256,16,16] ->[-1,1,16,16]\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self,filters=32):\n",
    "        super().__init__()\n",
    "        self.filters = filters\n",
    "        self.d1 = self.conv4(3,filters,norm=False) #[2, 32, 128, 128]\n",
    "        self.d2 = self.conv4(filters,2*filters) #[2, 64, 64, 64]\n",
    "        self.d3 = self.conv4(2*filters,4*filters) #[2, 128, 32, 32]\n",
    "        self.d4 = self.conv4(4*filters,8*filters) #[2, 256, 16, 16]\n",
    "        self.d5 = nn.Sequential(\n",
    "                nn.Conv2d(8*filters,1,kernel_size=3,stride=1,padding=1),\n",
    "            ) #[2, 1, 32, 32]\n",
    "        # 손실 함수 생성\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        # 옵티마이저 생성\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0005)\n",
    "    def forward(self,x):\n",
    "        y = self.d1(x)\n",
    "        y = self.d2(y)\n",
    "        y = self.d3(y)\n",
    "        y = self.d4(y)\n",
    "        y = self.d5(y)\n",
    "        return y\n",
    "    def conv4(self,in_channels,out_channels,f_size=4,stride=2,norm=True):\n",
    "        if norm:\n",
    "            conv4 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels,kernel_size=f_size,stride=stride,padding=1),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2),\n",
    "            )\n",
    "        else:\n",
    "            conv4 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels,kernel_size=f_size,stride=stride,padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "            )\n",
    "        return conv4\n",
    "\n",
    "    def train(self,x_batch,label):\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred = self.forward(x_batch)\n",
    "        \n",
    "        loss = self.loss_function(y_pred,label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dummy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones([\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m256\u001b[39m,\u001b[39m256\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m d \u001b[39m=\u001b[39m D()\n\u001b[0;32m      3\u001b[0m d(dummy)\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'D' is not defined"
     ]
    }
   ],
   "source": [
    "dummy = torch.ones([2,3,256,256])\n",
    "d = D()\n",
    "d(dummy).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import cv2 as cv\n",
    "\n",
    "class apple2orangeDataLoader():\n",
    "    \n",
    "    def __init__(self, dataset_name='apple2orange', img_res=(256, 256)):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.img_res = img_res\n",
    "        pass\n",
    "    def load_batch(self, batch_size=1, is_testing=False):\n",
    "        data_type = \"train\" if not is_testing else \"test\"\n",
    "        path_A = glob('./data/%s/%sA/*' % (self.dataset_name, data_type))\n",
    "        path_B = glob('./data/%s/%sB/*' % (self.dataset_name, data_type))\n",
    "\n",
    "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
    "        total_samples = self.n_batches * batch_size\n",
    "\n",
    "        # Sample n_batches * batch_size from each path list so that model sees all\n",
    "        # samples from both domains\n",
    "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
    "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
    "\n",
    "        for i in range(self.n_batches-1):\n",
    "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
    "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img_A, img_B in zip(batch_A, batch_B):\n",
    "                img_A = self.imread(img_A)\n",
    "                img_B = self.imread(img_B)\n",
    "\n",
    "                img_A = cv.resize(img_A, self.img_res)\n",
    "                img_B = cv.resize(img_B, self.img_res)\n",
    "\n",
    "                if not is_testing and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "\n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield torch.FloatTensor(imgs_A.transpose(0,3,1,2)), torch.FloatTensor(imgs_B.transpose(0,3,1,2))  \n",
    "            \n",
    "    def imread(self, path):\n",
    "        img = cv.imread(path)\n",
    "        img =cv.cvtColor(img,cv.COLOR_RGB2BGR)\n",
    "        return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_A = D()\n",
    "d_B = D()\n",
    "u_A = UNET()\n",
    "u_B = UNET()\n",
    "dummy = torch.ones([2,3,256,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataloader = apple2orangeDataLoader()\n",
    "batch_size = 4\n",
    "patch = int(256/2**4)\n",
    "disc_patch = (1,patch,patch) #[1,16,16]\n",
    "\n",
    "real_label = torch.ones((batch_size,)+disc_patch)\n",
    "fake_label = torch.zeros((batch_size,)+disc_patch)\n",
    "\n",
    "dA_loss_hist = []\n",
    "dB_loss_hist = []\n",
    "gA_loss_hist =[]\n",
    "gB_loss_hist =[]\n",
    "\n",
    "epoch=1\n",
    "for iter,(imgs_A,imgs_B) in enumerate(dataloader.load_batch(batch_size=batch_size)):\n",
    "\n",
    "    fake_B = u_B(imgs_A).detach()\n",
    "    fake_A = u_A(imgs_B).detach()\n",
    "\n",
    "    d_B_loss = d_B.train(imgs_B,real_label)\n",
    "    d_B_fake_loss = d_B.train(fake_B,fake_label)\n",
    "    d_A_loss =d_A.train(imgs_A,real_label)\n",
    "    d_A_fake_loss = d_A.train(fake_A,fake_label)\n",
    "\n",
    "    recons_A = u_A(fake_B)\n",
    "    recons_B = u_B(fake_A)\n",
    "    g_A_rec_loss = u_A.train(recons_A,imgs_A)\n",
    "    g_B_rec_loss = u_B.train(recons_B,imgs_B)\n",
    "    \n",
    "    idt_A = u_A(imgs_A)\n",
    "    idt_B = u_B(imgs_B)\n",
    "    \n",
    "    g_A_idt_loss = u_A.train(idt_A,imgs_A)\n",
    "    g_B_idt_loss = u_B.train(idt_B,imgs_B)\n",
    "\n",
    "    total_d_A_loss =(d_A_loss.item()+d_A_fake_loss.item())/2\n",
    "    total_d_B_loss =(d_B_loss.item()+d_B_fake_loss.item())/2\n",
    "    dA_loss_hist.append(total_d_A_loss)\n",
    "    dB_loss_hist.append(total_d_B_loss)\n",
    "\n",
    "    total_g_A_loss =(g_A_rec_loss.item()+g_A_idt_loss.item())/2\n",
    "    total_g_B_loss =(g_B_rec_loss.item()+g_B_idt_loss.item())/2\n",
    "    gA_loss_hist.append(total_g_A_loss)\n",
    "    gB_loss_hist.append(total_g_B_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}: d_A_loss = {total_d_A_loss:.7f} | d_B_loss = {total_d_B_loss:.7f} | total_g_A_loss={total_g_A_loss:.7f} | total_g_B_loss={total_g_B_loss:.7f}\")\n",
    "    if iter==1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(loss_hist):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.scatter(np.arange(1,len(loss_hist)+1),loss_hist,s=0.5)\n",
    "    plt.title('generator loss')\n",
    "    plt.show()\n",
    "visualize(dA_loss_hist)\n",
    "visualize(dB_loss_hist)\n",
    "visualize(gA_loss_hist)\n",
    "visualize(gB_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(data_loader):\n",
    "    \n",
    "    r, c = 2, 4\n",
    "    for iter,(imgs_A,imgs_B) in enumerate(dataloader.load_batch(batch_size=1)):\n",
    "\n",
    "        fake_B = u_B(imgs_A).detach()\n",
    "        fake_A = u_A(imgs_B).detach()\n",
    "\n",
    "        recons_A = u_A(fake_B)\n",
    "        recons_B = u_B(fake_A)\n",
    "        \n",
    "        idt_A = u_A(imgs_A)\n",
    "        idt_B = u_B(imgs_B)\n",
    "        gen_imgs = torch.concatenate([imgs_A, fake_B, recons_A, idt_A, imgs_B, fake_A, recons_B, idt_B]).detach().numpy().transpose((0,2,3,1))\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "\n",
    "        titles = ['Original', 'Translated', 'Reconstructed', 'ID']\n",
    "        fig, axs = plt.subplots(r, c, figsize=(25,12.5))\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[j])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "sample_images(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "747fe91b7bc9ec9d8624ac8c139c41948fb906c2c40d5ffbc4d71da454373257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
