{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"my_ae_trained.pth\"))\n",
    "torch.save(ae.state_dict(), \"my_ae_trained.pth\")\n",
    "#torch.save(model.state_dict(),\"./mount/My Drive/Colab Notebooks/GDLgon/my_vae_celeb_trained.pth\")\n",
    "\n",
    "vae.load_state_dict(torch.load(\"my_vae_trained.pth\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_input = np.load(\"MNIST_x_train.npy\").astype(np.float32).reshape(-1, 28*28)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.cuda.FloatTensor(self.x_input[idx])\n",
    "\n",
    "data_loader = DataLoader(MyDataset(), batch_size=1000, shuffle=True,\\\n",
    "    generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('./mount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "  print(\"using cuda:\", torch.cuda.get_device_name(0))\n",
    "  pass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    train_data = pd.read_csv(\"./mount/My Drive/Colab Notebooks/firstGAN/mnist_train.csv\")\n",
    "    train_data =train_data.to_numpy()\n",
    "    # read csv\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(MnistDataset.train_data)\n",
    "    def __getitem__(self, index):\n",
    "        #return torch.tensor(MnistDataset.train_data[index,1:]/255,dtype=torch.float32)\n",
    "        return torch.cuda.FloatTensor(MnistDataset.train_data[index,1:]/255.0)\n",
    "\n",
    "    def plot_image(self,n):\n",
    "        data = MnistDataset.train_data[n,1:].reshape(28,28)\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated(device) / (1024*1024*1024))\n",
    "print(torch.cuda.max_memory_allocated(device) / (1024*1024*1024))\n",
    "print(torch.cuda.memory_summary(device, abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy \n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self,dim_latent = 2):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784,128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,2*dim_latent),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(dim_latent,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,784),\n",
    "        )\n",
    "        self.dim_latent = dim_latent\n",
    "        self.optim_en = Adam(self.encoder.parameters(),lr=0.02)\n",
    "        self.optim_de = Adam(self.decoder.parameters(),lr=0.02)\n",
    "        self.loss_fn = MSELoss()\n",
    "\n",
    "        self.num_epoch=400\n",
    "        self.hist_loss = []\n",
    "        self.hist_model = []\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x).view(-1,2,self.dim_latent)\n",
    "\n",
    "        self.mu = x[:, 0, :]\n",
    "        self.sigma = torch.exp(x[:, 1, :])\n",
    "        self.z = self.mu + self.sigma * torch.randn_like(self.mu)\n",
    "\n",
    "        de = self.decoder(self.z)\n",
    "        return de\n",
    "\n",
    "    def train(self,x_input):\n",
    "        for epoch in range(1, 401):\n",
    "            self.optim_en.zero_grad()\n",
    "            self.optim_de.zero_grad()\n",
    "            y_pred = self.forward(x_input)\n",
    "            \n",
    "            mse_loss = ((x_input - y_pred)**2).sum()\n",
    "            kl_loss = (self.sigma**2 + self.mu**2 - torch.log(self.sigma) - 0.5).sum()\n",
    "            loss = mse_loss + kl_loss\n",
    "\n",
    "            loss.backward()\n",
    "            self.optim_de.step()\n",
    "            self.optim_en.step()\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
    "\n",
    "    def visualize(self):\n",
    "        pass\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "747fe91b7bc9ec9d8624ac8c139c41948fb906c2c40d5ffbc4d71da454373257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
