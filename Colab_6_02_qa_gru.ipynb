{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 데이터 전처리 부분은 책 코드 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21204,"status":"ok","timestamp":1677473846689,"user":{"displayName":"김정곤","userId":"13832456333438157121"},"user_tz":-540},"id":"fxmTfsN4Kyn3","outputId":"2a84b76d-41b7-4add-9abe-b4025780aaf6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at ./mount\n"]}],"source":["from google.colab import drive\n","drive.mount('./mount')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3203,"status":"ok","timestamp":1677473852974,"user":{"displayName":"김정곤","userId":"13832456333438157121"},"user_tz":-540},"id":"A4O3Kkh0LIDE","outputId":"925bb535-0e4a-45d8-a7e1-6b9fea342dac"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","if torch.cuda.is_available():\n","  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","  print(\"using cuda:\", torch.cuda.get_device_name(0))\n","  pass\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1677473960252,"user":{"displayName":"김정곤","userId":"13832456333438157121"},"user_tz":-540},"id":"Ycu3t7jnPIKm","outputId":"d2bde198-7577-492d-f623-78725acbfc4e"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'.'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.path.curdir"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"elapsed":10,"status":"error","timestamp":1677474051421,"user":{"displayName":"김정곤","userId":"13832456333438157121"},"user_tz":-540},"id":"0T_6VRPJLIAy","outputId":"c4aef485-da65-4870-c257-97cf458d1ebc"},"outputs":[{"ename":"SyntaxError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-18beaee2933e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from .mount.My Drive.Colab Notebooks.utils.write import training_data, test_data, collapse_documents, expand_answers, _read_data, glove\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["from .mount.My Drive.Colab Notebooks.utils.write import training_data, test_data, collapse_documents, expand_answers, _read_data, glove\n","training_data_gen = training_data()\n","# training_data_gen = [next(training_data_gen)]\n","test_data_gen = test_data()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tyi12nELH-l"},"outputs":[],"source":["t = next(training_data_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgxWVLjgLH8j"},"outputs":[],"source":["idx = 0\n","\n","print('document_tokens\\n', t['document_tokens'][idx])\n","print('\\n')\n","print('question_input_tokens\\n', t['question_input_tokens'][idx])\n","print('\\n')\n","print('answer_masks\\n', t['answer_masks'][idx])\n","print('\\n')\n","print('answer_labels\\n', t['answer_labels'][idx])\n","print('\\n')\n","print('question_output_tokens\\n', t['question_output_tokens'][idx])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XM7PTfD-LH5q"},"outputs":[],"source":["# GloVe\n","\n","VOCAB_SIZE = glove.shape[0]\n","EMBEDDING_DIMENS = glove.shape[1]\n","\n","print('GLOVE')\n","print('VOCAB_SIZE: ', VOCAB_SIZE)\n","print('EMBEDDING_DIMENS: ', EMBEDDING_DIMENS)\n","\n","GRU_UNITS = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Guvewac2LH3V"},"outputs":[],"source":["import numpy as np\n","import torch\n","from collections import Counter\n","import re\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import RMSprop,Adam\n","\n","class answerMask(nn.Module):\n","    def __init__(self,seq_length=200):\n","        super().__init__()\n","        self.seq_length = 200    \n","        self.n_units = 128\n","        embedding_size = self.n_units #n_units에 맞추기\n","        total_words = 9984 # gloVe dim에 맞추기\n","        self.num_layers = 1 #적층 레이어를 위해\n","        self.batch_size =1\n","\n","        self.embed = nn.Embedding(\n","            num_embeddings=total_words, embedding_dim= embedding_size\n","        )\n","        self.answer_mask_gru = nn.GRU(\n","            input_size=self.n_units,\n","            hidden_size=self.n_units,\n","            batch_first=True,\n","            dropout=0.2,            \n","        )\n","        self.answer_mask = nn.Sequential(\n","            nn.Linear(self.n_units,2), \n","            nn.Softmax(), \n","        )\n","\n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.optimizer = Adam(self.parameters(),lr=0.001)\n","    def forward(self,x,prev_state):  \n","        # [126, 200]\n","        print(\"x.shape\",x.shape)\n","        emb_init = self.embed(x) \n","        # [126, 200, 128] #prev_state [1, 126, 128]을 원함\n","        print(\"emb_init.shape\",emb_init.shape)\n","        self.output,state =self.answer_mask_gru(emb_init,prev_state)\n","        # [126, 200, 128] [1, 126, 128]\n","        print(\"self.output.shape\",self.output.shape)\n","        print(\"state.shape\",state.shape)\n","        mask = self.answer_mask(self.output)\n","        # [126, 200, 2]\n","        print(\"mask.shape\",mask.shape)\n","        return mask \n","\n","    def init_state(self):\n","        return torch.zeros(self.num_layers, self.batch_size, self.n_units)\n","    \n","    def build_loss(self,document,answer_label):\n","        self.batch_size = document.shape[0]\n","        state_h = self.init_state()\n","        self.optimizer.zero_grad()\n","\n","        y_pred = self.forward(document, state_h)\n","        y_pred = y_pred.view(-1,2)\n","        loss = self.loss_fn(y_pred, answer_label)\n","\n","        # loss.backward()\n","        # self.optimizer.step()\n","\n","        return loss\n","        \n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MV1V-98FLH1N"},"outputs":[],"source":["class QnAEncoderDecoderModel(nn.Module):\n","    def __init__(self,seq_length=20):\n","        super().__init__()\n","        \n","        # n_units = 256\n","        # embedding_size = 100\n","        self.seq_length =200\n","        self.n_units = 128\n","        self.num_layers = 1 #적층 레이어를 위해\n","        self.total_words = 9984 # gloVe dim에 맞추기\n","        \n","        self.encoder_gru = nn.GRU(\n","            input_size=self.n_units,\n","            hidden_size=self.n_units,\n","            batch_first=True,\n","            dropout=0.2,            \n","        )\n","\n","        self.decoder_gru = nn.GRU(\n","            input_size=self.n_units,\n","            hidden_size=self.n_units,\n","            batch_first=True,\n","            dropout=0.2,            \n","        )\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(self.n_units,self.total_words),\n","            nn.Softmax(),\n","        )      \n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.optimizer = Adam(self.parameters(),lr=0.001)\n","        self.dec_state =None #나중에 질문 생성을 위해\n","\n","    def forward(self,encoder_inputs,question,prev_state):  \n","        # encoder_inputs [126, 25, 128]  #h_0 [1, 126, 128]을 원함. [1, 25, 128]\n","        print(\"encoder_inputs.shape\",encoder_inputs.shape)\n","        print(\"question.shape\",question.shape)\n","        enc_output,enc_state = self.encoder_gru(encoder_inputs,prev_state)\n","        # [126, 25, 128], [1, 126, 128]\n","        print(\"enc_output.shape\",enc_output.shape)\n","        print(\"enc_state.shape\",enc_state.shape)\n","        # question [126, 13]\n","        emb_question = self.embed(question).detach()\n","        # [126, 13, 128]\n","        print(\"emb_question.shape\",emb_question.shape)\n","        dec_output,self.dec_state = self.decoder_gru( emb_question ,enc_state)\n","        # [126, 13, 128], [1, 126, 128]\n","        print(\"dec_output.shape\",dec_output.shape)\n","        print(\"dec_state.shape\",self.dec_state.shape)\n","\n","        logits = self.fc(dec_output) \n","        print(\"logits.shape\",logits.shape)\n","        #[126, 13, 9984]\n","        return logits\n","    \n","    def init_state(self):\n","        return torch.zeros(self.num_layers, self.batch_size, self.n_units)\n","\n","    def answerMaskDotAnswerOutputs(self,answer_mask_label,answer_outputs):\n","        # [119, 51, 200] x [119, 200, 512]\n","        encoder_inputs = torch.bmm(answer_mask_label,answer_outputs)\n","        # [119, 51, 512]\n","        return encoder_inputs\n","\n","    def predictY(self,answer_mask_model,question_input_token,answer_mask_label):\n","        self.embed = answer_mask_model.embed\n","        \n","        encoder_inputs = self.answerMaskDotAnswerOutputs(answer_mask_label,answer_mask_model.output)\n","        \n","        self.batch_size=encoder_inputs.shape[0]\n","        state_h = self.init_state()\n","        self.optimizer.zero_grad()\n","        \n","        y_pred = self.forward(encoder_inputs,question_input_token, state_h)\n","        return y_pred\n","\n","    def build_loss(self,answer_mask_model,question_input_token,question_output_token,answer_mask_label):\n","        y_pred = self.predictY(answer_mask_model,question_input_token,answer_mask_label)\n","        y_pred = y_pred.view(-1,self.total_words)\n","        loss = self.loss_fn(y_pred, question_output_token)\n","\n","        # loss.backward()\n","        # self.optimizer.step()\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EGynsRWLHy9"},"outputs":[],"source":["answer_mask_model = answerMask()\n","qna_model = QnAEncoderDecoderModel()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDUNrabpLHw4"},"outputs":[],"source":["import numpy as np\n","\n","training_loss_hist = []\n","test_loss_hist = []\n","\n","#EPOCHS = 2000\n","EPOCHS = 1\n","start_epoch = 1\n","for epoch in range(start_epoch, start_epoch + EPOCHS + 1):\n","    print(\"에포크 {0}\".format(epoch))\n","    \n","    for i, batch in enumerate(training_data()):\n","        \n","        val_batch = next(test_data_gen, None)\n","        \n","        if val_batch is None:\n","            test_data_gen = test_data()\n","            val_batch = next(test_data_gen, None)\n","        print('hi')\n","        answer_training_loss = answer_mask_model.build_loss(\n","            torch.LongTensor( batch['document_tokens']),\n","            torch.LongTensor(batch['answer_labels'].flatten()),\n","        )\n","        print('===hi===')\n","        decoder_training_loss = qna_model.build_loss(\n","            answer_mask_model,\n","            torch.LongTensor(batch['question_input_tokens']),\n","            torch.LongTensor(batch['question_output_tokens'].flatten()),\n","            torch.FloatTensor( batch['answer_masks']),\n","        )\n","        \n","    \n","        \n","        total_loss = answer_training_loss+ decoder_training_loss\n","        training_loss_hist.append(total_loss.item())\n","        total_loss.backward()\n","        answer_mask_model.optimizer.step()\n","        qna_model.optimizer.step()\n","\n","\n","        print(f\"{i}: 훈련 손실: {total_loss.item():.4f}\")    \n","        if i==1:\n","            break\n","#    if epoch % 100 == 0:\n","#        print(f\"{i}: 훈련 손실: {total_loss:.4f}\")    \n","    break"]},{"cell_type":"markdown","metadata":{"id":"biA4SXhyLUYK"},"source":["## 테스트  \n","gan gdl 참고 후 수정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nZk_WFxLHuz"},"outputs":[],"source":["test_data_gen = test_data()\n","batch = next(test_data_gen)\n","batch = collapse_documents(batch)\n","idx=0\n","print(batch['document_words'][idx][37:50])\n","for i in range(len(batch['document_words'][idx])):\n","    print(i, batch['document_words'][idx][i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVg0mtocLHtG"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# 대답 위치 예측\n","\n","plt.figure(figsize=(15,5))\n","idx = 0\n","\n","answer_mask_model.batch_size = torch.LongTensor( batch['document_tokens']).shape[0]\n","answer_preds = answer_mask_model(torch.LongTensor( batch['document_tokens']),answer_mask_model.init_state())\n","\n","print('예측한 대답의 확률')\n","ax = plt.gca()\n","ax.xaxis.grid(True)\n","plt.plot(answer_preds[idx, :, 1].detach().numpy())\n","plt.show()\n","\n","for i in range(len(batch['document_words'][idx])):\n","    print(i, batch['document_words'][idx][i], np.round(answer_preds[idx][i][1].detach().numpy(),2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3HiWjkwLHqm"},"outputs":[],"source":["# 대답 위치 선정\n","\n","start_answer = 37\n","end_answer = 39\n","\n","print(batch['document_words'][idx][start_answer:(1+end_answer)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QfWaTT2LHoc"},"outputs":[],"source":["from utils.write import START_TOKEN, END_TOKEN, look_up_token\n","\n","answer_mask_model.batch_size = torch.LongTensor( batch['document_tokens']).shape[0]\n","answer_preds = answer_mask_model(torch.LongTensor( batch['document_tokens']),answer_mask_model.init_state())\n","\n","answers = [[0] * len(answer_preds[idx])]\n","for i in range(start_answer, end_answer + 1):\n","    answers[0][i] = 1\n","\n","answer_batch = expand_answers(batch, answers)\n","with torch.no_grad():\n","    qna_model.predictY(\n","        answer_mask_model,\n","        torch.LongTensor( answer_batch['document_tokens'][[idx]]), \n","        torch.FloatTensor(answer_batch['answer_masks'][[idx]]))\n","    next_decoder_init_state = qna_model.dec_state\n","    print(\"next_decoder_init_state.shape\",next_decoder_init_state.shape)\n","\n","    word_tokens = [START_TOKEN]\n","    questions = [look_up_token(START_TOKEN)]\n","\n","    ended = False\n","    counter = 0\n","\n","    while not ended:\n","        \n","        counter += 1\n","        #print(\"torch.LongTensor(word_tokens)\",torch.LongTensor(word_tokens).unsqueeze(1).shape)\n","        emb_output=qna_model.embed(torch.LongTensor(word_tokens).unsqueeze(1))\n","        #print(\"emb_output\",emb_output.shape)\n","        decoder_output, next_decoder_init_state = qna_model.decoder_gru( \n","            emb_output,\n","            next_decoder_init_state)\n","        \n","        word_preds = qna_model.fc(decoder_output)\n","        #print(\"word_preds.shape\",word_preds.shape)\n","        word_tokens = torch.argmax(word_preds, 2)[0]\n","        print(\"look_up_token(word_tokens[0].detach().numpy())\",look_up_token(word_tokens[0].detach().numpy()))\n","        questions.append(look_up_token(word_tokens[0].detach().numpy()))\n","\n","        if word_tokens[0] == END_TOKEN or counter > 20 :\n","            ended = True\n","\n","    questions = ' '.join(questions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_TM-BOOLdGo"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPlTowscE4Rcc1Oz+pomTwm","provenance":[]},"kernelspec":{"display_name":"torch","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.16"},"vscode":{"interpreter":{"hash":"747fe91b7bc9ec9d8624ac8c139c41948fb906c2c40d5ffbc4d71da454373257"}}},"nbformat":4,"nbformat_minor":0}
